# Eduardo Cortes

**Continuity of Mind in Language Agents**  
Persistent cognition, memory, and interpretable control loops for LLM-based systems.

---

## Research Thesis

I design language agents that maintain evolving internal state across time, rather than operating as stateless prompt-completion systems. My work focuses on episodic and conceptual memory, reflective control loops, and interpretability methods that make cognition inspectable and steerable. The goal is to couple performance and transparency in long-horizon reasoning systems.

---

## Cognitive Stack

**Perception**
- Multimodal inputs (text, vision) via CLIP-style encoders and language models

**Memory**
- Episodic memory: temporally ordered experience and belief trajectories
- Conceptual memory: abstracted invariants learned from episodes

**Control**
- Meta-agent loops for planning, reflection, and tool use
- System 1 / System 2 separation for fluency versus constraint

**Interpretability**
- Sparse feature representations
- Concept activation, steering, and drift monitoring

**Environments**
- Interactive tasks (e.g., ALFWorld-style agents)
- Formal systems (Lean 4 theorem proving)
- Generative perception pipelines (graphics and vision)

---

## Selected Work

- **Continuity of Mind:** https://github.com/project194-cognitivellm/cognitivellm/tree/eduardo
    Episodic and conceptual memory system for long-horizon language agents.  
  Experience abstraction, belief persistence, and retrieval.
  Architectural experiments in persistent cognition and reflective loops.  
  Explores how structural constraints reduce uncertainty at decode time.

- **Ghosts in the Loop**  
  Proxy tuning and logit-level interventions for reasoning enhancement.  
  Studies the interpretability–performance feedback loop.

- **Saliency-Guided Rendering**  
  CLIP-based saliency maps for adaptive perception in graphics pipelines.

---

## Open Questions

- How does persistent memory reshape the geometry of latent space in LLMs?
- Can interpretability constraints improve sample efficiency in reasoning tasks?
- What abstractions naturally emerge from episodic experience?
- How should uncertainty propagate across long-horizon agent loops?

---

EECS @ UC Berkeley · Language Agents · Interpretability · Multimodal Reasoning
